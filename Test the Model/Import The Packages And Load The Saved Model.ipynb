{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8768daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.7.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.3.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (2.0.7)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (3.7.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.16.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (14.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.50.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (4.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (0.37.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.21.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (2.10.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (1.14.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (3.19.6)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorflow==2.7.1) (0.27.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (2.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (1.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.1) (63.4.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.1) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.1) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.1) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.1) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.1) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd21e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa5e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Datagen\n",
    "train_datagen = ImageDataGenerator(rescale=1/255,zoom_range=0.2,horizontal_flip=True,vertical_flip=False)\n",
    "# Testing Datagen\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75925b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721c68b",
   "metadata": {},
   "source": [
    "# Apply ImageDataGenerator Functionality To Train And Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253121d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has been created and uploaded by IBM-TeamID-PNT2022TMID29575\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "print(\"This dataset has been created and uploaded by IBM-TeamID-PNT2022TMID29575\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffcb39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,zoom_range=0.2,horizontal_flip=True, vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429bb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen= ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781ff4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15750 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\training_set',target_size=(64,64), batch_size=300,\n",
    "                                          class_mode='categorical', color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ff9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917b12d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2250 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_test = test_datagen.flow_from_directory(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\test_set',target_size=(64,64), batch_size=300,\n",
    "                                          class_mode='categorical', color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbbe880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb227edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fc455",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3efc7",
   "metadata": {},
   "source": [
    "# Import The Required Model Building Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f59238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0358f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c796b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301be100",
   "metadata": {},
   "source": [
    "IMPORTING tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4858b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c9bda",
   "metadata": {},
   "source": [
    "# Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8e0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #to view graph in colab itself\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700b741",
   "metadata": {},
   "source": [
    "Applying ImageDataGenerator to training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "384d032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15750 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\training_set',target_size=(64,64), batch_size=300,\n",
    "                                          class_mode='categorical', color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953dc939",
   "metadata": {},
   "source": [
    "Applying ImageDataGenerator to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_datagen.flow_from_directory(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\test_set',target_size=(64,64), batch_size=300,\n",
    "                                          class_mode='categorical', color_mode = \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12e4f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=len(x_train)\n",
    "b=len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bafa1",
   "metadata": {},
   "source": [
    "Length of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9fcd839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4219921",
   "metadata": {},
   "source": [
    "Length of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc8f557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d0977",
   "metadata": {},
   "source": [
    "# Add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "769e8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f3a06",
   "metadata": {},
   "source": [
    "Add The Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cf497fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32,(3,3),input_shape=(64,64,1),activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787aaf63",
   "metadata": {},
   "source": [
    "Add Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9d4d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188fd69",
   "metadata": {},
   "source": [
    "Add The Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a395265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b82c0",
   "metadata": {},
   "source": [
    "Adding The Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa705aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=512,activation='relu'))\n",
    "model.add(Dense(units=261,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b0d944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=9,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1399d",
   "metadata": {},
   "source": [
    "Compile The Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef4fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc98627",
   "metadata": {},
   "source": [
    "Fit The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daca9098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singa\\AppData\\Local\\Temp\\ipykernel_72356\\234118701.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(x_train,steps_per_epoch=len(x_train),epochs=10,validation_data=x_test,validation_steps=len(x_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 225s 4s/step - loss: 0.5389 - accuracy: 0.8278 - val_loss: 0.2257 - val_accuracy: 0.9516\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 36s 674ms/step - loss: 0.0619 - accuracy: 0.9834 - val_loss: 0.2191 - val_accuracy: 0.9649\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 35s 653ms/step - loss: 0.0270 - accuracy: 0.9933 - val_loss: 0.1810 - val_accuracy: 0.9764\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 37s 692ms/step - loss: 0.0134 - accuracy: 0.9966 - val_loss: 0.2009 - val_accuracy: 0.9778\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 33s 618ms/step - loss: 0.0080 - accuracy: 0.9983 - val_loss: 0.1671 - val_accuracy: 0.9782\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 41s 784ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.1880 - val_accuracy: 0.9787\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 33s 623ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.2251 - val_accuracy: 0.9773\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 35s 659ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.1773 - val_accuracy: 0.9773\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 35s 660ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.1767 - val_accuracy: 0.9782\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 37s 690ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 0.1991 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19054c10520>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(x_train,steps_per_epoch=len(x_train),epochs=10,validation_data=x_test,validation_steps=len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d99d23",
   "metadata": {},
   "source": [
    "Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "822a7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('asl.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20c6b2",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e19f04",
   "metadata": {},
   "source": [
    "Import The Packages And Load The Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f205e7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "     ---------------------------------------- 35.6/35.6 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\singa\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21d6c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d49da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('asl.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06c41553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=image.load_img(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\test_set/C/10.png',target_size=(400,500))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "905bba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x400>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=image.load_img(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\test_set/D/10.png',target_size=(400,500))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3eba75a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAGQCAIAAADX0QWRAAAITUlEQVR4nO3dPWpV7RqA4W8fFFJY+FOopZXai8QB+BNwAA5AUYfgDGzEwkbR3hGIig4gFtZmBtHCn0Zi5T7l4ZC1OWdrslb2nesqXyQ8iN48kJd3/fMPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAKppNPQARL1++HDy/devWUj/nypUrg+ebm5tLzwSH2L+mHgCAvSfuAEHiDhAk7gBB4g4QJO4AQa5Cspz5fD71CP9x8uTJwfPv378v9XNOnDgxeP7t27elZ9rlwYMHg+cPHz78+x8Oi9jcAYLEHSBI3AGCxB0gSNwBgtyWYaEDdTFm0MePHwfPL126NHj+4cOHwfPLly/v2Uz/t0VPoS16Og2WYnMHCBJ3gCBxBwgSd4AgcQcIcltmGltbW4Pn58+f//sf/uLFi8HzO3fuDJ7fvXt38Pzp06d/PwzLGnxzZtEDNbCIzR0gSNwBgsQdIEjcAYLEHSDoyNQDxE3yPMvt27eXOgd6bO4AQeIOECTuAEHiDhAk7gBB4g4Q5OGwvfH79+/B89nM3zB7wD8klmVzBwgSd4AgcQcIEneAIHEHCPJw2NI+ffq0+9BlBvbV2tra4PmvX79GnoRVYXMHCBJ3gCBxBwgSd4AgcQcIclsGVsDOzs5Sf/7s2bOD558/f96LcVgBNneAIHEHCBJ3gCBxBwgSd4AgL6IsbT6fTz0C/KGNjY3B8zdv3ow8CfvN5g4QJO4AQeIOECTuAEHiDhAk7gBBrkIuzVVIenwnssfmDhAk7gBB4g4QJO4AQeIOEOQzewutr69PPQKMZNEdMLdoVpfNHSBI3AGCxB0gSNwBgsQdIMhtmYUeP3489QgAf8jmDhAk7gBB4g4QJO4AQeIOEOS2zEKnT5+eegSAP2RzBwgSd4AgcQcIEneAIHEHCBJ3gCDf0Fpo0YfH4PDwmb3VZXMHCBJ3gCBxBwgSd4AgcQcI8nAYsNCpU6d2H379+nX8SViWzR0gSNwBgsQdIEjcAYLEHSDIbRlgoaNHj049An/I5g4QJO4AQeIOECTuAEHiDhDktgyw0PHjx3cffv78efRBWJrNHSBI3AGCxB0gSNwBgsQdIEjcAYJmUw9wcM3n86lHgIkdO3Zs9+HPnz/Hn4Rl2dwBgsQdIEjcAYLEHSBI3AGC3JZZyG0ZmM0kYlXZ3AGCxB0gSNwBgsQdIEjcAYLEHSBI3AGCxB0gSNwBgsQdIEjcAYKOTD3AwbW5uTl4vr6+PvIkMJULFy7sPtza2hp/EpZlcwcIEneAIHEHCBJ3gCBxBwgSd4AgVyGBhdx6XF02d4AgcQcIEneAIHEHCBJ3gKDZ1AOsnvl8PvUIMJLZTCJWlc0dIEjcAYLEHSBI3AGCxB0gSNwBgsQdIEjcAYLEHSBI3AGCxB0gSNwBgsQdIEjcAYLEHSBI3AGCxB0gSNwBgsQdIEjcAYLEHSBI3AGCxB0gSNwBgsQdIEjcAYLEHSBI3AGCxB0gSNwBgo5MPcDq2dra2n144cKF8ScBWMTmDhAk7gBB4g4QJO4AQeIOECTuAEGuQgL/nDlzZuoR2GM2d4AgcQcIEneAIHEHCBJ3gCC3ZZb2/Pnz3YePHj0afxLYK+fOnRs8//Lly8iTsFds7gBB4g4QJO4AQeIOECTuAEGzqQeImM/nU48A/9vGxsbg+Zs3b0aehP1mcwcIEneAIHEHCBJ3gCBxBwhyW2ZvuC3DSnj16tXg+c2bN0eehP1mcwcIEneAIHEHCBJ3gCBxBwgSd4AgVyH3xvb29uD5mTNnRp4E/sBsJgU1NneAIHEHCBJ3gCBxBwgSd4CgI1MPAIxn0Wf26LG5AwSJO0CQuAMEiTtAkLgDBHlQYn/5/B4HyokTJwbPf/z4Me4g7DubO0CQuAMEiTtAkLgDBIk7QJDbMvvLbRkOFF9cOjxs7gBB4g4QJO4AQeIOECTuAEHiDhDkM3twiFy/fn3w/O3btyNPwn6zuQMEiTtAkLgDBIk7QJC4AwS5LbO/fv36NXi+trY28iTAoWJzBwgSd4AgcQcIEneAIHEHCPLNrWn4/B6T8Jm9w8PmDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEFeEZrGos/s7ezsjDwJSR4Iw+YOECTuAEHiDhAk7gBB4g4Q5FfqB4vP77Enrl69Onj+/v37kSdhKjZ3gCBxBwgSd4AgcQcIEneAoCNTD8B/OXv27OD59vb2yJOwEp48eTJ47lYMNneAIHEHCBJ3gCBxBwgSd4Agb8usBm/OsBRfYsLmDhAk7gBB4g4QJO4AQeIOECTuAEEeDlsNFy9e3H346dOn8SfhQLl///7UI3BA2dwBgsQdIEjcAYLEHSBI3AGC3JZZDV+/fp16BGCV2NwBgsQdIEjcAYLEHSBI3AGCfItrhfn2Hj6nxyI2d4AgcQcIEneAIHEHCBJ3gCBxBwgSd4AgcQcIEneAIHEHCBJ3gCBxBwjy6lCQB8UOj3v37g2eP3v2bORJOGhs7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHelgnytgyzmf/ah53NHSBI3AGCxB0gSNwBgsQdIMiv1INu3LgxeP769euRJ2EqbstgcwcIEneAIHEHCBJ3gCBxBwgSd4Ag96UOEQ+KHR6uQmJzBwgSd4AgcQcIEneAIHEHCPIr9UPEbRmuXbs2eP7u3buRJ2G/2dwBgsQdIEjcAYLEHSBI3AGC/g2Lhso5jq5ETwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x400>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=image.load_img(r'\\Users\\singa\\Desktop\\IBM AI project\\conversation engine for deaf and dumb\\Dataset\\test_set/E/10.png',target_size=(400,500))\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
